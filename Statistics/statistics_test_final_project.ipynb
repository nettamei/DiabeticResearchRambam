{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Global"
      ],
      "metadata": {
        "id": "U-u7WV0ROaGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "import pandas as pd\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.spatial.distance import cosine\n",
        "from statsmodels.stats.proportion import proportions_ztest"
      ],
      "metadata": {
        "id": "gNaxYRLfu64L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KL"
      ],
      "metadata": {
        "id": "WcvT_oSLu73F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85nKJ6LOpcq9"
      },
      "outputs": [],
      "source": [
        "# Function to calculate KL divergence between two distributions\n",
        "def calculate_kl_divergence(dist1, dist2):\n",
        "      return entropy(dist1, dist2)\n",
        "\n",
        "def KL(df,epsilon=1e-6):\n",
        "  # Drop the id column for aggregation\n",
        "  pattern_data = df.drop(columns=['id', 'class_name'])\n",
        "\n",
        "  kl_results = []\n",
        "  for i in range(10):\n",
        "    # Split the data randomly into two groups (half and half)\n",
        "    random_indices = np.random.permutation(len(pattern_data))\n",
        "    mid_index = len(pattern_data) // 2\n",
        "    group1_indices = random_indices[:mid_index]\n",
        "    group2_indices = random_indices[mid_index:]\n",
        "\n",
        "    group1 = pattern_data.iloc[group1_indices]\n",
        "    group2 = pattern_data.iloc[group2_indices]\n",
        "\n",
        "    # Aggregate pattern counts within each group\n",
        "    group1_aggregated = group1.sum(axis=0) + epsilon\n",
        "    group2_aggregated = group2.sum(axis=0) + epsilon\n",
        "    # Normalize the aggregated pattern counts to create valid probability distributions\n",
        "    group1_distribution = group1_aggregated / group1_aggregated.sum()\n",
        "    group2_distribution = group2_aggregated / group2_aggregated.sum()\n",
        "\n",
        "\n",
        "    # Compute KL divergence between the two group-level distributions\n",
        "    kl_results.append(calculate_kl_divergence(group1_distribution, group2_distribution))\n",
        "\n",
        "  return np.round(np.mean(kl_results), 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def KL_diversity(group1, group2,epsilon=1e-6):\n",
        "  group1 = group1.drop(columns=['id', 'class_name'])\n",
        "  group2 = group2.drop(columns=['id', 'class_name'])\n",
        "  # Aggregate pattern counts within each group\n",
        "  group1_aggregated = group1.sum(axis=0) + epsilon\n",
        "  group2_aggregated = group2.sum(axis=0)  +epsilon\n",
        "\n",
        "  # Normalize the aggregated pattern counts to create valid probability distributions\n",
        "  group1_distribution = group1_aggregated / group1_aggregated.sum()\n",
        "  group2_distribution = group2_aggregated / group2_aggregated.sum()\n",
        "\n",
        "\n",
        "  # Compute KL divergence between the two group-level distributions\n",
        "  return calculate_kl_divergence(group1_distribution, group2_distribution)"
      ],
      "metadata": {
        "id": "SjqEdvPTz8Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KS"
      ],
      "metadata": {
        "id": "Oejzd7Yn8gBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KS(df,epsilon=1e-6):\n",
        "  # Drop the id column for aggregation\n",
        "  pattern_data = df.drop(columns=['id', 'class_name'])\n",
        "  ks_results = []\n",
        "  p_val = []\n",
        "  results = {'Fold': [], 'KS Statistic': [], 'P-Value': [], 'Result': []}\n",
        "  for i in range(10):\n",
        "    # Split the data randomly into two groups (half and half)\n",
        "    random_indices = np.random.permutation(len(pattern_data))\n",
        "    mid_index = len(pattern_data) // 2\n",
        "    group1_indices = random_indices[:mid_index]\n",
        "    group2_indices = random_indices[mid_index:]\n",
        "\n",
        "    group1 = pattern_data.iloc[group1_indices]\n",
        "    group2 = pattern_data.iloc[group2_indices]\n",
        "\n",
        "    # Aggregate pattern counts within each group\n",
        "    group1_aggregated = group1.sum(axis=0)  + epsilon\n",
        "    group2_aggregated = group2.sum(axis=0) + epsilon\n",
        "\n",
        "    # Normalize to create comparable distributions\n",
        "    group1_norm = group1_aggregated / group1_aggregated.sum()\n",
        "    group2_norm = group2_aggregated / group2_aggregated.sum()\n",
        "\n",
        "    # Perform KS test\n",
        "    ks_statistic, p_value = ks_2samp(group1_norm, group2_norm)\n",
        "\n",
        "    if p_value > 0.05:\n",
        "      r = 'similar'\n",
        "    else:\n",
        "      r = 'not similar'\n",
        "\n",
        "    results['Fold'].append(i)\n",
        "    results['KS Statistic'].append(ks_statistic)\n",
        "    results['P-Value'].append(p_value)\n",
        "    results['Result'].append(r)\n",
        "\n",
        "  return pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "nR5-MG7pET4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KS_test(group1, group2, epsilon=1e-6):\n",
        "  group1 = group1.drop(columns=['id', 'class_name'])\n",
        "  group2 = group2.drop(columns=['id', 'class_name'])\n",
        "  # Aggregate pattern counts within each group\n",
        "  group1_aggregated = group1.sum(axis=0) +epsilon\n",
        "  group2_aggregated = group2.sum(axis=0) + epsilon\n",
        "\n",
        "  # Normalize for consistency\n",
        "  group1_norm = group1_aggregated / group1_aggregated.sum()\n",
        "  group2_norm = group2_aggregated / group2_aggregated.sum()\n",
        "\n",
        "  # Run Kolmogorov-Smirnov test\n",
        "  ks_statistic, p_value = ks_2samp(group1_norm, group2_norm)\n",
        "\n",
        "  # Interpretation\n",
        "  if p_value > 0.05:\n",
        "    r = \"similar\"\n",
        "  else:\n",
        "    r = \"not similar\"\n",
        "\n",
        "  result = {\"KS Statistic\": [ks_statistic], \"P-Value\": [p_value], \"Result\": [r]}\n",
        "  return pd.DataFrame(result)\n"
      ],
      "metadata": {
        "id": "LmYuoieZHB-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate"
      ],
      "metadata": {
        "id": "Y84wrkAoO-x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def in_groups(dfs):\n",
        "  for name, df in dfs:\n",
        "    kl_result = KL(df) # one number\n",
        "    ks_result = KS(df) # return df\n",
        "    print(f\"KL within {name}: {kl_result}\")\n",
        "    ks_result.to_csv(f\"{name}_ks_within.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6gGKUCqnZlT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def between_groups(groups):\n",
        "  for name, group1, group2 in groups:\n",
        "    print(name)\n",
        "    kl_result = KL_diversity(group1, group2) # number\n",
        "    ks_result = KS_test(group1, group2)\n",
        "    print(f\"KL between {name}: {kl_result}\")\n",
        "    ks_result.to_csv(f\"{name}_ks_between.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2VnIW68JZpb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "# fill null as 0\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "numeric_cols = list(numeric_cols)[1:] # remove 'id' col\n",
        "\n",
        "# Min-Max Normalization\n",
        "for col in numeric_cols:\n",
        "    min_col = df[col].min()\n",
        "    max_col = df[col].max()\n",
        "    if max_col - min_col == 0:\n",
        "        df[col] = 0\n",
        "    else:\n",
        "        df[col] = (df[col] - min_col) / (max_col - min_col)\n",
        "\n",
        "#  Extract relevant columns\n",
        "\n",
        "meta_cols = [col for col in ['id', 'class_name'] if col in df.columns]\n",
        "# hs_cols =  [col for col in df.columns if col.endswith('_Type:HorizontalSupport')]\n",
        "hs_cols =  [col for col in df.columns if col.endswith('_Type:MeanDuration')]\n",
        "\n",
        "\n",
        "# Split into positive and negative class\n",
        "true_df = df[df['class_name'] == True]\n",
        "false_df = df[df['class_name'] == False]\n",
        "\n",
        "true_df = true_df.reindex(columns=meta_cols + hs_cols, fill_value=0)\n",
        "false_df = false_df.reindex(columns=meta_cols + hs_cols, fill_value=0)"
      ],
      "metadata": {
        "id": "964QpYxwZvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dfs = [(\"Hypoglicemia\", true_df), (\"not-hypoglicemia\", false_df)]\n",
        "dfs = [(\"Heart_Attack\", true_df), (\"not-Heart_Attack\", false_df)]\n",
        "# dfs = [(\"hyperglycemia_over_180\", true_df), (\"not_hyperglycemia_over_180\", false_df)]\n",
        "# dfs = [(\"hyperglycemia_over_200\", true_df), (\"not_hyperglycemia_over_200\", false_df)]\n",
        "\n",
        "\n",
        "in_groups(dfs)\n",
        "\n",
        "\n",
        "# between groups\n",
        "# Hypo vs not Hypo\n",
        "# groups = [(\"Hypoglicemia\", true_df, false_df)]\n",
        "groups = [(\"Heart_Attack\", true_df, false_df)]\n",
        "# groups = [(\"hyperglycemia_over_180\", true_df, false_df)]\n",
        "# groups = [(\"hyperglycemia_over_200\", true_df, false_df)]\n",
        "\n",
        "between_groups(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmy9uoKwotfZ",
        "outputId": "b1d76ab6-8628-4fbf-fc6e-cb0682fb45e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL within Heart_Attack: 0.006\n",
            "KL within not-Heart_Attack: 0.001\n",
            "Heart_Attack\n",
            "KL between Heart_Attack: 0.010704278029007242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proportion"
      ],
      "metadata": {
        "id": "rpBmU63JOU8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "between"
      ],
      "metadata": {
        "id": "LuduTtKixdjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "for name, group1, group2 in groups:\n",
        "    test_results = []\n",
        "    g1 = group1.drop(columns=['id', 'class_name'])\n",
        "    g2 = group2.drop(columns=['id', 'class_name'])\n",
        "\n",
        "    total1 = g1[hs_cols].sum().sum()\n",
        "    total2 = g2[hs_cols].sum().sum()\n",
        "\n",
        "    for column in hs_cols:\n",
        "        ratio1 = g1[column].sum() / total1 if total1 > 0 else 0\n",
        "        ratio2 = g2[column].sum() / total2 if total2 > 0 else 0\n",
        "\n",
        "        diff = abs(ratio1 - ratio2)\n",
        "\n",
        "        threshold = 0.0005\n",
        "        similar = 'similar' if diff <= threshold else 'not similar'\n",
        "\n",
        "        test_results.append((column, ratio1, ratio2, diff, similar))\n",
        "\n",
        "    results_df = pd.DataFrame(test_results, columns=['Column', 'Ratio1', 'Ratio2', 'AbsDiff', 'Similarity'])\n",
        "\n",
        "    not_similar_count = results_df[results_df['Similarity'] == 'not similar'].shape[0]\n",
        "    total_count = results_df.shape[0]\n",
        "    percentage_not_similar = (not_similar_count / total_count) * 100\n",
        "\n",
        "    print(f\"{name} (Horizontal Proportion): Not Similar Patterns: {percentage_not_similar:.1f}%\")\n",
        "\n",
        "    top_diff = results_df.sort_values(by='AbsDiff', ascending=False).head(10)\n",
        "    top_diff[['Column', 'Ratio1', 'Ratio2', 'AbsDiff']].to_csv(f\"top_10_significant_patterns_horizontal_proportion_{name}.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "acJ-ZPjCOdAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c24fd51-67e2-41ce-c12f-209cf84a4434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heart_Attack (Horizontal Proportion): Not Similar Patterns: 28.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "within"
      ],
      "metadata": {
        "id": "NOJfKnpHxgFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = {\"name\": [], \"Fold\": [], \"similar_percent\": []}\n",
        "\n",
        "THRESHOLD = 0.0005\n",
        "\n",
        "for name, df in dfs:\n",
        "    g = df.drop(columns=['id', 'class_name'])\n",
        "    for i in range(10):\n",
        "        test_results = []\n",
        "\n",
        "        random_indices = np.random.permutation(len(g))\n",
        "        mid_index = len(g) // 2\n",
        "        group1 = g.iloc[random_indices[:mid_index]]\n",
        "        group2 = g.iloc[random_indices[mid_index:]]\n",
        "\n",
        "        total1 = group1[hs_cols].sum().sum()\n",
        "        total2 = group2[hs_cols].sum().sum()\n",
        "\n",
        "        for column in hs_cols:\n",
        "            ratio1 = group1[column].sum() / total1 if total1 > 0 else 0\n",
        "            ratio2 = group2[column].sum() / total2 if total2 > 0 else 0\n",
        "            diff = abs(ratio1 - ratio2)\n",
        "\n",
        "            similar = 'similar' if diff <= THRESHOLD else 'not similar'\n",
        "            test_results.append((column, ratio1, ratio2, diff, similar))\n",
        "\n",
        "        results_df = pd.DataFrame(test_results, columns=['Column', 'Ratio1', 'Ratio2', 'AbsDiff', 'Similarity'])\n",
        "\n",
        "        similar_count = results_df[results_df['Similarity'] == 'similar'].shape[0]\n",
        "        total_count = results_df.shape[0]\n",
        "        similar_percent = (similar_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "        results['name'].append(name)\n",
        "        results['Fold'].append(i)\n",
        "        results['similar_percent'].append(similar_percent)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"proportion_within_meanDuration.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "douL-uglXLPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}